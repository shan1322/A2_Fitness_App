{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ab8492-8086-4297-85e3-0261a791fea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Images\n",
      "Tim Spector\n",
      "THE DIET MYTH\n",
      "The Real Science\n",
      "Behind What We Eat\n",
      "Images\n",
      "Dedication\n",
      "To my family and other microbes\n",
      "C o n t e n t s\n",
      " Title Page\n",
      " Dedication\n",
      " Introduction: A Bad Taste\n",
      "  1Not on the Label: Microbes\n",
      "  2Energy and Calories\n",
      "  3Fats: Total\n",
      "  4Fats: Saturated\n",
      "  5Fats: Unsaturated\n",
      "  6Trans Fats\n",
      "  7Protein: Animal\n",
      "  8Protein: Non-animal\n",
      "  9Protein: Milk Products\n",
      "10Carbohydrates: of which Sugars\n",
      "11Carbohydrates: Non-sugars\n",
      "12Fibre\n",
      "13Artificial Sweeteners and Preservatives\n",
      "14Contains C\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "pdf_text = extract_text_from_pdf(\"data/papers/diet_myth.pdf\")\n",
    "print(pdf_text[:500])  # Print first 500 characters to verify extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "738cd152-685a-45fb-ab82-363c7c809ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nImages\\nTim Spector\\nTHE DIET MYTH\\nThe Real Science\\nBehind What We Eat\\nImages\\nDedication\\nTo my family and other microbes\\nC o n t e n t s\\n\\xa0Title Page\\n\\xa0Dedication\\n\\xa0Introduction: A Bad Taste\\n\\xa0\\xa01Not on the Label: Microbes\\n\\xa0\\xa02Energy and Calories\\n\\xa0\\xa03Fats: Total\\n\\xa0\\xa04Fats: Saturated\\n\\xa0\\xa05Fats: Unsaturated\\n\\xa0\\xa06Trans Fats\\n\\xa0\\xa07Protein: Animal\\n\\xa0\\xa08Protein: Non-animal\\n\\xa0\\xa09Protein: Milk Products\\n10Carbohydrates: of which Sugars\\n11Carbohydrates: Non-sugars\\n12Fibre\\n13Artificial Sweeteners and Preservatives\\n14Contains Cocoa and Caffeine\\n15Contains Alcohol\\n16Vitamins\\n17Warning: May Contain Antibiotics\\n18Warning: May Contain Nuts\\n19Best-before Date\\n\\xa0Conclusion: The Checkout\\n\\xa0\\n\\xa0Glossary\\n\\xa0Acknowledgements\\n\\xa0Notes\\n\\xa0By the same author\\n\\xa0Copyright\\nIntroduction\\nA Bad Taste\\nIt had been a tough climb: six hours walking up 1,200 metres to the summit on\\ntouring skis with artificial sealskins to stop us sliding backwards on the snow. Like my five companions I was feeling tired and a bit light-headed but I still\\nwanted to check out the spectacular view at 3,100 metres over Bormio on the\\nItalian–Austrian border. We had been ski-touring in the area for the past six\\ndays, staying in high-altitude mountain lodges, enjoying plenty of exercise and\\ngood Italian food.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_and_split_text(text, chunk_size=3):\n",
    "    sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", text)\n",
    "    chunks = [\" \".join(sentences[i:i+chunk_size]) for i in range(0, len(sentences), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "# Example usage\n",
    "text_chunks = clean_and_split_text(pdf_text)\n",
    "print(text_chunks[:1])  # Show first 3 chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bb6b587-c36f-4803-8437-85ae9b9946a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Load a Hugging Face model (BERT variant for sentence embeddings)\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # Works with TensorFlow\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = TFAutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8832a624-3fe7-430c-8f0b-a1f38b1dc080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    return np.mean(outputs.last_hidden_state.numpy(), axis=1)  # Take mean pooling\n",
    "\n",
    "# Convert research text chunks into embeddings\n",
    "chunk_embeddings = np.array([get_embedding(chunk) for chunk in text_chunks])\n",
    "shape_e=chunk_embeddings.shape\n",
    "chunk_embeddings=chunk_embeddings.reshape(shape_e[0],shape_e[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d564531-15d1-4de8-959a-069bca7490c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9df09e1-7370-4754-aa73-a373104fb61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "983\n",
      "Chunk Embeddings Shape: (1639, 384)\n",
      "Number of Text Chunks: 1639\n",
      "Similarities shape: (1639,)\n",
      "0.8050833\n",
      "This elegant series of experiments shows that artificial sweeteners are\n",
      "definitely not a free lunch; they do have potentially harmful metabolic effects\n",
      "that can increase weight gain and the risk of diabetes. They do this because\n",
      "even so-called inert chemicals can be crucial for our microbes, which change\n",
      "their function and so affect our bodies. We don’t yet know the true extent of\n",
      "the risk of the sweeteners and whether everyone is susceptible, but these\n",
      "microbe experiments have made sure that we and our food regulators who\n",
      "approve the new ‘safe compounds’, once they pass the cancer tests, should now\n",
      "take these risks more seriously.\n"
     ]
    }
   ],
   "source": [
    "def process_user_query(user_question, text_chunks, chunk_embeddings,get_embedding,threshold=0.5):\n",
    "    best_match, score = find_relevant_text(user_question, text_chunks, chunk_embeddings,get_embedding)\n",
    "    print(score)\n",
    "    if score < threshold:\n",
    "        return \"Information not available in research papers.\"\n",
    "    else:\n",
    "        return best_match\n",
    "\n",
    "# Example usage\n",
    "print(process_user_query(\"Do artificial sweeteners cause cancer?\", text_chunks, chunk_embeddings,get_embedding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b14d0f7-f3c6-4b61-942f-4a29b7609014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, tokenizer, and embeddings saved successfully!\n"
     ]
    }
   ],
   "source": [
    "## save every thing to make compute easy\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "\n",
    "# Define save directory\n",
    "save_dir = \"/models/nlp/\"\n",
    "#os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Load a Hugging Face model (BERT variant for sentence embeddings)\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = TFAutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"models/nlp/bert_model\")\n",
    "tokenizer.save_pretrained(\"models/nlp/bert_tokenizer\")\n",
    "\n",
    "# Example embeddings (Replace with actual embeddings)\n",
    "embeddings = chunk_embeddings  # 100 embeddings of size 384\n",
    "\n",
    "# Save embeddings\n",
    "np.save(\"models/nlp/embeddings.npy\", embeddings)\n",
    "\n",
    "print(\"Model, tokenizer, and embeddings saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af55f58d-0e60-4c05-adcc-f1b176b4239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"models/nlp/text_chunks.npy\",text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af1d5e8c-d061-4b40-adbe-250fca8f3f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertModel.\n",
      "\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at models/nlp/bert_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, tokenizer, and embeddings loaded successfully! Embeddings shape: (1639, 384)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = TFAutoModel.from_pretrained(\"models/nlp/bert_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/nlp/bert_tokenizer\")\n",
    "\n",
    "# Load embeddings\n",
    "chunk_embeddings = np.load(\"models/nlp/embeddings.npy\")\n",
    "\n",
    "\n",
    "print(f\"Model, tokenizer, and embeddings loaded successfully! Embeddings shape: {embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97de43c-8628-457e-872b-f61e84a6c302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e9801d5-e8cf-422d-91e4-67cb3a68df41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities shape: (1639,)\n",
      "Chunk Embeddings Shape: (1639, 384)\n",
      "Number of Text Chunks: 1639\n",
      "Best Match (Score: 0.4809572696685791):\n",
      "These mechanisms are complex, and regardless of\n",
      "whether they are due to gut or brain chemicals or psychological factors (as we\n",
      "learned in the introductory story of twins on diets) they are strongly influenced\n",
      "by genes as well as by microbes. It is hard to distinguish in high-protein diets between how much the benefits\n",
      "and side effects can be put down to the lack of carbs and how much to the\n",
      "extra protein. During the recent evolution of the Atkins Diet the high priests\n",
      "of the billion-dollar corporation behind it have increasingly stressed the\n",
      "importance of low rather than zero carbs and reduced the emphasis on heavy\n",
      "meat eating.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def find_relevant_text(user_question, text_chunks, chunk_embeddings, get_embedding):\n",
    "    # Convert user question into an embedding\n",
    "    question_embedding = get_embedding(user_question).flatten()\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarities = np.dot(chunk_embeddings, question_embedding)\n",
    "\n",
    "    # Ensure similarity shape matches the number of text chunks\n",
    "    assert similarities.shape[0] == len(text_chunks), \"Mismatch in similarity scores and text chunks!\"\n",
    "\n",
    "    # Normalize (avoid division by zero)\n",
    "    chunk_norms = norm(chunk_embeddings, axis=1)\n",
    "    question_norm = norm(question_embedding)\n",
    "    similarities = similarities / (chunk_norms * question_norm + 1e-8)\n",
    "\n",
    "    # Print shapes for debugging\n",
    "    print(f\"Similarities shape: {similarities.shape}\")\n",
    "    print(f\"Chunk Embeddings Shape: {chunk_embeddings.shape}\")\n",
    "    print(f\"Number of Text Chunks: {len(text_chunks)}\")\n",
    "\n",
    "    # Get the best matching chunk\n",
    "    best_match_index = np.argmax(similarities)\n",
    "\n",
    "    # Ensure index is within bounds\n",
    "    if best_match_index >= len(text_chunks):\n",
    "        raise IndexError(f\"Index {best_match_index} is out of bounds! Check text chunk and embedding sizes.\")\n",
    "\n",
    "    best_match_score = similarities[best_match_index]\n",
    "    return text_chunks[best_match_index], best_match_score\n",
    "\n",
    "# Example usage\n",
    "user_question = \"Does eating carbs at night cause weight gain?\"\n",
    "best_match, score = find_relevant_text(user_question, text_chunks, chunk_embeddings, get_embedding)\n",
    "print(f\"Best Match (Score: {score}):\\n{best_match}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8037fc57-9dcf-48ed-b9c1-de4ed1c80426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aea733-7d1d-4ff5-a402-c44334937a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
